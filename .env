# Environment configuration for KYC Vision Extraction Phase 1
# Adjust values as needed. Restart the server after changes.

# Base URL where Ollama (OpenAI-compatible) listens
# OLLAMA_BASE_URL=http://localhost:11434

# Vision-capable local model name (ensure it's pulled: `ollama pull minicpm-v`)
VISION_MODEL=meta-llama/llama-4-scout-17b-16e-instruct
# VISION_MODEL=minicpm-v
# VISION_MODEL=minicpm-v:latest
# VISION_MODEL=llava:7b
# VISION_MODEL=llava:7b   
# VISION_MODEL=gemma3:4b
# VISION_MODEL=Qwen/Qwen2.5-VL-7B-Instruct

# # Maximum upload size in megabytes
# MAX_FILE_MB=15

# # Maximum number of PDF pages to render (extra pages ignored + truncated=true)
# MAX_PAGES_RENDER=4

# # Enable verbose model & prompt debugging (0/1)
# DEBUG_EXTRACTION=1

#GROQ_API_KEY=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
GROQ_API_KEY=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
# HF_TOKEN=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